{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import module1\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'ttk_train'\n",
    "df = module1.read_df_from_csv(filename)\n",
    "df_test = module1.read_df_from_csv('ttk_test_etalon')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_lite......... PanLex Lite Corpus\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "Hit Enter to continue: \n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [P] all-corpora......... All the corpora\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> punkt\n",
      "    Downloading package punkt to /home/parkin/nltk_data...\n",
      "      Unzipping tokenizers/punkt.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('russian')\n",
    "#we can add stop words\n",
    "\n",
    "import pymorphy2\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text_list = nltk.word_tokenize(text)\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    text_list = [word for word in text_list if word[0] != '@' and len(word) > 1 and word.isalpha()]\n",
    "    text_list = [morph.parse(word)[0].normal_form for word in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tf-idf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "j = 0\n",
    "token_list = [i for i in df['text']]\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words)\n",
    "tfs = tfidf.fit_transform(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_filter(_df, _tfs, corporation):\n",
    "    x_arr = []\n",
    "    y_arr = []\n",
    "    for i in range(len(_df)):\n",
    "        label = _df[corporation][i]\n",
    "        if label == '0' or label == '-1' or label == '1' or label == 0.0 or label == -1.0 or label == 1.0:\n",
    "            x_arr.append(_tfs[i].toarray()[0])\n",
    "            y_arr.append(int(label))\n",
    "    X = np.array(x_arr)\n",
    "    y = np.array(y_arr)\n",
    "    print (len(X))\n",
    "    print (len(y))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "2005\n",
      "1198\n",
      "1198\n",
      "310\n",
      "310\n",
      "520\n",
      "520\n",
      "989\n",
      "989\n",
      "3\n",
      "3\n",
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "companies = ['mts', 'megafon', 'tele2', 'rostelecom', 'beeline', 'komstar', 'skylink']\n",
    "X_train, y_train = df_filter(_df=df, _tfs=tfs, corporation=companies[0])\n",
    "for companie in companies[1:]:\n",
    "    X_curr, y_curr = df_filter(_df=df, _tfs=tfs, corporation=companie)\n",
    "    X_train = np.append(X_train, X_curr, axis=0)\n",
    "    y_train = np.append(y_train, y_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1454\n",
      "1454\n",
      "139\n",
      "139\n",
      "32\n",
      "32\n",
      "17\n",
      "17\n",
      "2530\n",
      "2530\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "companies = ['mts', 'megafon', 'tele2', 'rostelecom', 'beeline', 'komstar', 'skylink']\n",
    "X_test, y_test = df_filter(_df=df_test, _tfs=tfs, corporation=companies[0])\n",
    "for companie in companies[1:]:\n",
    "    X_curr, y_curr = df_filter(_df=df_test, _tfs=tfs, corporation=companie)\n",
    "    if len(X_curr) > 0:\n",
    "        X_test = np.append(X_test, X_curr, axis=0)\n",
    "        y_test = np.append(y_test, y_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "SVM: \n",
      "fit time = 7m0sec\n",
      "predict time = 5m38sec\n",
      "f-macro: 0.26861258167596697\n",
      "f-micro: 0.6748142822909178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print ('SVM: ')\n",
    "start_time = time.time()\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "t = int(time.time() - start_time)\n",
    "print ('fit time = {0}m{1}sec'.format(t // 60, t % 60))\n",
    "start_time = time.time()\n",
    "y_predict = model.predict(X_test)\n",
    "t = int(time.time() - start_time)\n",
    "print ('predict time = {0}m{1}sec'.format(t // 60, t % 60))\n",
    "print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange f score - 0.4282238442822384\n"
     ]
    }
   ],
   "source": [
    "import my_score\n",
    "func= lambda x,y: my_score.f_macro(my_score.cound_diff(x,y))\n",
    "print ('strange f score - {0}'.format(func(y_predict, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Decision Tree: \n",
      "fit time = 0.31666666666666665 19\n",
      "predict time = 0.0 0\n",
      "f-macro: 0.3118088716037848\n",
      "f-micro: 0.4215192906781692\n",
      "strange f score - 0.3986225467350215\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "AdaBoost DecisionTree max_depth = 2, n = 100: \n",
      "fit time = 6.35 21\n",
      "predict time = 0.08333333333333333 5\n",
      "f-macro: 0.3153888646236782\n",
      "f-micro: 0.43829379343398034\n",
      "strange f score - 0.4131220255348981\n"
     ]
    }
   ],
   "source": [
    "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print ('Decision Tree: ')\n",
    "start_time = time.time()\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "t = int(time.time() - start_time)\n",
    "print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "start_time = time.time()\n",
    "y_predict = model.predict(X_test)\n",
    "t = int(time.time() - start_time)\n",
    "print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "print ('strange f score - {0}'.format(func(y_predict, y_test)))\n",
    "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print ('AdaBoost DecisionTree max_depth = 2, n = 100: ')\n",
    "start_time = time.time()\n",
    "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=100,\n",
    "learning_rate=1)\n",
    "model.fit(X_train, y_train)\n",
    "t = int(time.time() - start_time)\n",
    "print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "start_time = time.time()\n",
    "y_predict = model.predict(X_test)\n",
    "t = int(time.time() - start_time)\n",
    "print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "print ('strange f score - {0}'.format(func(y_predict, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 6.583333333333333m 35sec\n"
     ]
    }
   ],
   "source": [
    "#countVectorizer\n",
    "start_time = time.time()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1, tokenizer=tokenize, stop_words=stop_words)\n",
    "tfs4vec = vectorizer.fit_transform(token_list)\n",
    "t = int(time.time() - start_time)\n",
    "print ('TIME: {0}m {1}sec'.format(t / 60, t % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = module1.read_df_from_csv('ttk_test_etalon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bdcd962e52f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX4_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs4vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX4_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4_test_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs4vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcompanies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megafon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tele2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rostelecom'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_filter' is not defined"
     ]
    }
   ],
   "source": [
    "X_all, y_all = df_filter(_df = df, _tfs = tfs, corporation = 'beeline')\n",
    "X4_all, y4_all = df_filter(_df = df, _tfs = tfs4vec, corporation = 'beeline')\n",
    "X_test_all, y_test_all = df_filter(_df = df_test, _tfs = tfs, corporation = 'beeline')\n",
    "X4_test_all, y4_test_all = df_filter(_df = df_test, _tfs = tfs4vec, corporation = 'beeline')\n",
    "companies = ['mts', 'megafon', 'tele2', 'rostelecom']\n",
    "for companie in companies:\n",
    "    print('!!!!!!!!!!!!!!!!!')\n",
    "    print(companie)\n",
    "    print ('~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    X, y = df_filter(_df = df, _tfs = tfs, corporation = companie)\n",
    "    X_all = np.vstack((X_all, X))\n",
    "    y_all = np.concatenate((y_all,y))\n",
    "    X4, y4 = df_filter(_df = df, _tfs = tfs4vec, corporation = companie)\n",
    "    X4_all = np.vstack((X4_all, X4))\n",
    "    y4_all = np.concatenate((y4_all,y4))\n",
    "    X_test, y_test = df_filter(_df = df_test, _tfs = tfs, corporation = companie)\n",
    "    X_test_all = np.vstack((X_test_all, X_test))\n",
    "    y_test_all = np.concatenate((y_test_all,y_test))\n",
    "    X4_test, y4_test = df_filter(_df = df_test, _tfs = tfs4vec, corporation = companie)\n",
    "    X4_test_all = np.vstack((X4_test_all, X4_test))\n",
    "    y4_test_all = np.concatenate((y4_test_all,y4_test))\n",
    "    \n",
    "\n",
    "def calculation(X, y, X_test, y_test, X4, y4, X4_test, y4_test):    \n",
    "    index = int((X.shape[0] + X_test.shape[0]) * 0.2)\n",
    "    if (index < X_test.shape[0]):\n",
    "        X = np.vstack((X, X_test[index:]))\n",
    "        X_test = X_test[:index]\n",
    "        y = np.concatenate((y,y_test[index:]))\n",
    "        y_test = y_test[:index]\n",
    "        X4 = np.vstack((X4, X4_test[index:]))\n",
    "        X4_test = X4_test[:index]\n",
    "        y4 = np.concatenate((y4,y4_test[index:]))\n",
    "        y4_test = y4_test[:index]\n",
    "    \n",
    "    print ('DATA SIZE: TRAIN = {0}, TEST= {1}'.format(X.shape[0], X_test.shape[0]))\n",
    "    print ('Naive Bayes: ')\n",
    "    start_time = time.time()\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    t = int(time.time() - start_time)\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('SVM: ')\n",
    "    start_time = time.time()\n",
    "    model = SVC()\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model = SVC()\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))  \n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('SVM linear kernel: ')\n",
    "    start_time = time.time()\n",
    "    model = SVC(kernel = 'linear')\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model = SVC(kernel = 'linear')\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('kNN 10: ')\n",
    "    start_time = time.time()\n",
    "    model = KNeighborsClassifier(n_neighbors=10)\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  KNeighborsClassifier(n_neighbors=10)\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('kNN 5: ')\n",
    "    start_time = time.time()\n",
    "    model = KNeighborsClassifier(n_neighbors=5)\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  KNeighborsClassifier(n_neighbors=5)\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('kNN 3: ')\n",
    "    start_time = time.time()\n",
    "    model = KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('kNN 7: ')\n",
    "    start_time = time.time()\n",
    "    model = KNeighborsClassifier(n_neighbors=7)\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  KNeighborsClassifier(n_neighbors=7)\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('kNN 30: ')\n",
    "    start_time = time.time()\n",
    "    model = KNeighborsClassifier(n_neighbors=30)\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  KNeighborsClassifier(n_neighbors=30)\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('Decision Tree: ')\n",
    "    start_time = time.time()\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  DecisionTreeClassifier()\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('Decision Tree max depth = 5: ')\n",
    "    start_time = time.time()\n",
    "    model = DecisionTreeClassifier(max_depth = 5)\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  DecisionTreeClassifier(max_depth = 5)\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print ('AdaBoost DecisionTree max_depth = 2, n = 100: ')\n",
    "    start_time = time.time()\n",
    "    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100,\n",
    "    learning_rate=1)\n",
    "    model.fit(X, y)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X_test)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
    "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
    "    print('~~~~~count vectorizer~~~~~~')\n",
    "    start_time = time.time()\n",
    "    model =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100,\n",
    "    learning_rate=1)\n",
    "    model.fit(X4, y4)\n",
    "    t = int(time.time() - start_time)\n",
    "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
    "    start_time = time.time()\n",
    "    y_predict = model.predict(X4_test)\n",
    "    t = int(time.time() - start_time) \n",
    "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
    "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
    "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ba8c43bb4c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcalculation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX4_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my4_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX4_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX4_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my4_test_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'calculation' is not defined"
     ]
    }
   ],
   "source": [
    "calculation(X = X_all, y = y_all, X_test = X_test_all, y_test = y_test_all, X4 = X4_all, y4 = y4_all, X4_test = X4_test_all, y4_test = y4_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
