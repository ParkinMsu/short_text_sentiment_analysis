{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "",
  "signature": "sha256:28a3e7647fa2d9e53627aec137740293f1186320ca4964f584819049d1a889d4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import module1\n",
      "import nltk\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import time\n",
      "\n",
      "from sklearn import preprocessing\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import accuracy_score\n",
      "from os import path\n",
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subdir = './datasets/twitter/'\n",
      "filename = 'ttk_train.csv'\n",
      "#df = module1.read_df_from_csv(filename)\n",
      "df = pd.read_csv(path.join(subdir,filename))\n",
      "#df_test = module1.read_df_from_csv('ttk_test_etalon')\n",
      "df_test = pd.read_csv(path.join(subdir, 'ttk_test_etalon.csv'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NLTK Downloader\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n",
        "Downloader> d\n",
        "\n",
        "Download which package (l=list; x=cancel)?\n",
        "  Identifier> \n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n",
        "Downloader> d\n",
        "\n",
        "Download which package (l=list; x=cancel)?\n",
        "  Identifier> l\n",
        "Packages:\n",
        "  [ ] abc................. Australian Broadcasting Commission 2006\n",
        "  [ ] alpino.............. Alpino Dutch Treebank\n",
        "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
        "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
        "  [ ] basque_grammars..... Grammars for Basque\n",
        "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
        "                           Extraction Systems in Biology)\n",
        "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
        "  [ ] book_grammars....... Grammars from NLTK Book\n",
        "  [ ] brown............... Brown Corpus\n",
        "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
        "  [ ] cess_cat............ CESS-CAT Treebank\n",
        "  [ ] cess_esp............ CESS-ESP Treebank\n",
        "  [ ] chat80.............. Chat-80 Data Files\n",
        "  [ ] city_database....... City Database\n",
        "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
        "  [ ] comparative_sentences Comparative Sentence Dataset\n",
        "  [ ] comtrans............ ComTrans Corpus Sample\n",
        "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
        "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
        "Hit Enter to continue: \n",
        "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
        "                           and Basque Subset)\n",
        "  [ ] crubadan............ Crubadan Corpus\n",
        "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
        "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
        "                           Corpus\n",
        "  [ ] floresta............ Portuguese Treebank\n",
        "  [ ] framenet_v15........ FrameNet 1.5\n",
        "  [ ] gazetteers.......... Gazeteer Lists\n",
        "  [ ] genesis............. Genesis Corpus\n",
        "  [ ] gutenberg........... Project Gutenberg Selections\n",
        "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
        "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
        "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
        "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
        "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
        "                           ChaSen format)\n",
        "  [ ] kimmo............... PC-KIMMO Data Files\n",
        "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
        "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
        "                           for parser comparison\n",
        "Hit Enter to continue: \n",
        "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
        "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
        "                           part-of-speech tags\n",
        "  [ ] machado............. Machado de Assis -- Obra Completa\n",
        "  [ ] masc_tagged......... MASC Tagged Corpus\n",
        "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
        "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
        "  [ ] moses_sample........ Moses Sample Models\n",
        "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
        "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
        "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
        "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
        "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
        "  [ ] nps_chat............ NPS Chat\n",
        "  [ ] omw................. Open Multilingual Wordnet\n",
        "  [ ] opinion_lexicon..... Opinion Lexicon\n",
        "  [ ] panlex_lite......... PanLex Lite Corpus\n",
        "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
        "  [ ] paradigms........... Paradigm Corpus\n",
        "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
        "                           Evaluation Shared Task\n",
        "Hit Enter to continue: \n",
        "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
        "                           character properties in Perl\n",
        "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
        "  [ ] pl196x.............. Polish language of the XX century sixties\n",
        "  [ ] porter_test......... Porter Stemmer Test Files\n",
        "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
        "  [ ] problem_reports..... Problem Report Corpus\n",
        "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
        "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
        "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
        "  [ ] pros_cons........... Pros and Cons\n",
        "  [ ] ptb................. Penn Treebank\n",
        "  [ ] punkt............... Punkt Tokenizer Models\n",
        "  [ ] qc.................. Experimental Data for Question Classification\n",
        "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
        "                           version\n",
        "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
        "                           Portuguesa)\n",
        "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
        "  [ ] sample_grammars..... Sample Grammars\n",
        "  [ ] semcor.............. SemCor 3.0\n",
        "Hit Enter to continue: \n",
        "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
        "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
        "  [ ] sentiwordnet........ SentiWordNet\n",
        "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
        "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
        "  [ ] smultron............ SMULTRON Corpus Sample\n",
        "  [ ] snowball_data....... Snowball Data\n",
        "  [ ] spanish_grammars.... Grammars for Spanish\n",
        "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
        "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
        "  [ ] swadesh............. Swadesh Wordlists\n",
        "  [ ] switchboard......... Switchboard Corpus Sample\n",
        "  [ ] tagsets............. Help on Tagsets\n",
        "  [ ] timit............... TIMIT Corpus Sample\n",
        "  [ ] toolbox............. Toolbox Sample Files\n",
        "  [ ] treebank............ Penn Treebank Sample\n",
        "  [ ] twitter_samples..... Twitter Samples\n",
        "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
        "                           (Unicode Version)\n",
        "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
        "  [ ] unicode_samples..... Unicode Samples\n",
        "Hit Enter to continue: \n",
        "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
        "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
        "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
        "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
        "  [ ] webtext............. Web Text Corpus\n",
        "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
        "  [ ] word2vec_sample..... Word2Vec Sample\n",
        "  [ ] wordnet............. WordNet\n",
        "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
        "  [ ] words............... Word Lists\n",
        "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
        "                           English Prose\n",
        "\n",
        "Collections:\n",
        "  [P] all-corpora......... All the corpora\n",
        "  [P] all................. All packages\n",
        "  [P] book................ Everything used in the NLTK Book\n",
        "\n",
        "([*] marks installed packages; [P] marks partially installed collections)\n",
        "\n",
        "Download which package (l=list; x=cancel)?\n",
        "  Identifier> punkt\n",
        "    Downloading package punkt to /home/parkin/nltk_data...\n",
        "      Unzipping tokenizers/punkt.zip.\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n",
        "Downloader> q\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "stop_words = stopwords.words('russian')\n",
      "#we can add stop words\n",
      "\n",
      "import pymorphy2\n",
      "def tokenize(text):\n",
      "    text = text.lower()\n",
      "    text_list = nltk.word_tokenize(text)\n",
      "    morph = pymorphy2.MorphAnalyzer()\n",
      "    text_list = [word for word in text_list if word[0] != '@' and len(word) > 1 and word.isalpha()]\n",
      "    text_list = [morph.parse(word)[0].normal_form for word in text_list]\n",
      "    return text_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tf-idf vectorizer\n",
      "j = 0\n",
      "token_list = [i for i in df['text']]\n",
      "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words)\n",
      "tfs = tfidf.fit_transform(token_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ngram_vectorize\n",
      "token_list = [ngram_vectorize.get_ngrams(i,n=3,is_join=True) for i in df['text']]\n",
      "tfidf = TfidfVectorizer()\n",
      "tfs = tfidf.fit_transform(token_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def df_filter(_df, _tfs, corporation):\n",
      "    x_arr = []\n",
      "    y_arr = []\n",
      "    for i in range(len(_df)):\n",
      "        label = _df[corporation][i]\n",
      "        if label == '0' or label == '-1' or label == '1' or label == 0.0 or label == -1.0 or label == 1.0:\n",
      "            x_arr.append(_tfs[i].toarray()[0])\n",
      "            y_arr.append(int(label))\n",
      "    X = np.array(x_arr)\n",
      "    y = np.array(y_arr)\n",
      "    print (len(X))\n",
      "    print (len(y))\n",
      "    return X, y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "companies = ['mts', 'megafon', 'tele2', 'rostelecom', 'beeline', 'komstar', 'skylink']\n",
      "X_train, y_train = df_filter(_df=df, _tfs=tfs, corporation=companies[0])\n",
      "for companie in companies[1:]:\n",
      "    X_curr, y_curr = df_filter(_df=df, _tfs=tfs, corporation=companie)\n",
      "    X_train = np.append(X_train, X_curr, axis=0)\n",
      "    y_train = np.append(y_train, y_curr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2005\n",
        "2005\n",
        "1198"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1198\n",
        "310"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "310\n",
        "520"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "520\n",
        "989"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "989\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3\n",
        "12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "12\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "companies = ['mts', 'megafon', 'tele2', 'rostelecom', 'beeline', 'komstar', 'skylink']\n",
      "X_test, y_test = df_filter(_df=df_test, _tfs=tfs, corporation=companies[0])\n",
      "for companie in companies[1:]:\n",
      "    X_curr, y_curr = df_filter(_df=df_test, _tfs=tfs, corporation=companie)\n",
      "    if len(X_curr) > 0:\n",
      "        X_test = np.append(X_test, X_curr, axis=0)\n",
      "        y_test = np.append(y_test, y_curr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1454\n",
        "1454\n",
        "139"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "139\n",
        "32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32\n",
        "17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "17\n",
        "2530"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2530\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "(5037, 22529)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "print ('SVM: ')\n",
      "start_time = time.time()\n",
      "model = Linear\n",
      "model.fit(X_train, y_train)\n",
      "t = int(time.time() - start_time)\n",
      "print ('fit time = {0}m{1}sec'.format(t // 60, t % 60))\n",
      "start_time = time.time()\n",
      "y_predict = model.predict(X_test)\n",
      "t = int(time.time() - start_time)\n",
      "print ('predict time = {0}m{1}sec'.format(t // 60, t % 60))\n",
      "print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "SVM: \n",
        "fit time = 7m38sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "predict time = 5m13sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "f-macro: 0.3211161797698344"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "f-micro: 0.4380541576803259\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import my_score\n",
      "func= lambda x,y: my_score.f_macro(my_score.cound_diff(x,y))\n",
      "print ('strange f score - {0}'.format(func(y_predict, y_test)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "strange f score - 0.4029549524727924\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "print ('Decision Tree: ')\n",
      "start_time = time.time()\n",
      "model = DecisionTreeClassifier()\n",
      "model.fit(X_train, y_train)\n",
      "t = int(time.time() - start_time)\n",
      "print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "start_time = time.time()\n",
      "y_predict = model.predict(X_test)\n",
      "t = int(time.time() - start_time)\n",
      "print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "print ('strange f score - {0}'.format(func(y_predict, y_test)))\n",
      "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "print ('AdaBoost DecisionTree max_depth = 5, n = 100: ')\n",
      "start_time = time.time()\n",
      "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=100,\n",
      "learning_rate=1)\n",
      "model.fit(X_train, y_train)\n",
      "t = int(time.time() - start_time)\n",
      "print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "start_time = time.time()\n",
      "y_predict = model.predict(X_test)\n",
      "t = int(time.time() - start_time)\n",
      "print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "print ('strange f score - {0}'.format(func(y_predict, y_test)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Decision Tree: \n",
        "fit time = 0.31666666666666665 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "predict time = 0.0 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "f-macro: 0.31121255793620656\n",
        "f-micro: 0.4188832973879703\n",
        "strange f score - 0.39824603339836173\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "AdaBoost DecisionTree max_depth = 2, n = 100: \n",
        "fit time = 12.716666666666667 43"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "predict time = 0.06666666666666667 4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "f-macro: 0.3115039620071108\n",
        "f-micro: 0.4205607476635514\n",
        "strange f score - 0.3993678096621752\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#countVectorizer\n",
      "start_time = time.time()\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "vectorizer = CountVectorizer(min_df=1, tokenizer=tokenize, stop_words=stop_words)\n",
      "tfs4vec = vectorizer.fit_transform(token_list)\n",
      "t = int(time.time() - start_time)\n",
      "print ('TIME: {0}m {1}sec'.format(t / 60, t % 60))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TIME: 6.583333333333333m 35sec\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = module1.read_df_from_csv('ttk_test_etalon')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_all, y_all = df_filter(_df = df, _tfs = tfs, corporation = 'beeline')\n",
      "X4_all, y4_all = df_filter(_df = df, _tfs = tfs4vec, corporation = 'beeline')\n",
      "X_test_all, y_test_all = df_filter(_df = df_test, _tfs = tfs, corporation = 'beeline')\n",
      "X4_test_all, y4_test_all = df_filter(_df = df_test, _tfs = tfs4vec, corporation = 'beeline')\n",
      "companies = ['mts', 'megafon', 'tele2', 'rostelecom']\n",
      "for companie in companies:\n",
      "    print('!!!!!!!!!!!!!!!!!')\n",
      "    print(companie)\n",
      "    print ('~~~~~~~~~~~~~~~~~')\n",
      "    \n",
      "    X, y = df_filter(_df = df, _tfs = tfs, corporation = companie)\n",
      "    X_all = np.vstack((X_all, X))\n",
      "    y_all = np.concatenate((y_all,y))\n",
      "    X4, y4 = df_filter(_df = df, _tfs = tfs4vec, corporation = companie)\n",
      "    X4_all = np.vstack((X4_all, X4))\n",
      "    y4_all = np.concatenate((y4_all,y4))\n",
      "    X_test, y_test = df_filter(_df = df_test, _tfs = tfs, corporation = companie)\n",
      "    X_test_all = np.vstack((X_test_all, X_test))\n",
      "    y_test_all = np.concatenate((y_test_all,y_test))\n",
      "    X4_test, y4_test = df_filter(_df = df_test, _tfs = tfs4vec, corporation = companie)\n",
      "    X4_test_all = np.vstack((X4_test_all, X4_test))\n",
      "    y4_test_all = np.concatenate((y4_test_all,y4_test))\n",
      "    \n",
      "\n",
      "def calculation(X, y, X_test, y_test, X4, y4, X4_test, y4_test):    \n",
      "    index = int((X.shape[0] + X_test.shape[0]) * 0.2)\n",
      "    if (index < X_test.shape[0]):\n",
      "        X = np.vstack((X, X_test[index:]))\n",
      "        X_test = X_test[:index]\n",
      "        y = np.concatenate((y,y_test[index:]))\n",
      "        y_test = y_test[:index]\n",
      "        X4 = np.vstack((X4, X4_test[index:]))\n",
      "        X4_test = X4_test[:index]\n",
      "        y4 = np.concatenate((y4,y4_test[index:]))\n",
      "        y4_test = y4_test[:index]\n",
      "    \n",
      "    print ('DATA SIZE: TRAIN = {0}, TEST= {1}'.format(X.shape[0], X_test.shape[0]))\n",
      "    print ('Naive Bayes: ')\n",
      "    start_time = time.time()\n",
      "    model = MultinomialNB()\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    t = int(time.time() - start_time)\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model = MultinomialNB()\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('SVM: ')\n",
      "    start_time = time.time()\n",
      "    model = SVC()\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model = SVC()\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))  \n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('SVM linear kernel: ')\n",
      "    start_time = time.time()\n",
      "    model = SVC(kernel = 'linear')\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model = SVC(kernel = 'linear')\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('kNN 10: ')\n",
      "    start_time = time.time()\n",
      "    model = KNeighborsClassifier(n_neighbors=10)\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  KNeighborsClassifier(n_neighbors=10)\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('kNN 5: ')\n",
      "    start_time = time.time()\n",
      "    model = KNeighborsClassifier(n_neighbors=5)\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  KNeighborsClassifier(n_neighbors=5)\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('kNN 3: ')\n",
      "    start_time = time.time()\n",
      "    model = KNeighborsClassifier(n_neighbors=3)\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  KNeighborsClassifier(n_neighbors=3)\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('kNN 7: ')\n",
      "    start_time = time.time()\n",
      "    model = KNeighborsClassifier(n_neighbors=7)\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  KNeighborsClassifier(n_neighbors=7)\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('kNN 30: ')\n",
      "    start_time = time.time()\n",
      "    model = KNeighborsClassifier(n_neighbors=30)\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  KNeighborsClassifier(n_neighbors=30)\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('Decision Tree: ')\n",
      "    start_time = time.time()\n",
      "    model = DecisionTreeClassifier()\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  DecisionTreeClassifier()\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('Decision Tree max depth = 5: ')\n",
      "    start_time = time.time()\n",
      "    model = DecisionTreeClassifier(max_depth = 5)\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  DecisionTreeClassifier(max_depth = 5)\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))\n",
      "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
      "    print ('AdaBoost DecisionTree max_depth = 2, n = 100: ')\n",
      "    start_time = time.time()\n",
      "    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100,\n",
      "    learning_rate=1)\n",
      "    model.fit(X, y)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X_test)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60))\n",
      "    print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))\n",
      "    print('~~~~~count vectorizer~~~~~~')\n",
      "    start_time = time.time()\n",
      "    model =  AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100,\n",
      "    learning_rate=1)\n",
      "    model.fit(X4, y4)\n",
      "    t = int(time.time() - start_time)\n",
      "    print ('fit time = {0} {1}'.format(t / 60, t % 60))\n",
      "    start_time = time.time()\n",
      "    y_predict = model.predict(X4_test)\n",
      "    t = int(time.time() - start_time) \n",
      "    print ('predict time = {0} {1}'.format(t / 60, t % 60)) \n",
      "    print ('f-macro: {0}'.format(f1_score(y4_test, y_predict, average = 'macro')))\n",
      "    print ('f-micro: {0}'.format(f1_score(y4_test, y_predict, average = 'micro')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'df_filter' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-bdcd962e52f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX4_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs4vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX4_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4_test_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_tfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfs4vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorporation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'beeline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcompanies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megafon'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tele2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rostelecom'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'df_filter' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calculation(X = X_all, y = y_all, X_test = X_test_all, y_test = y_test_all, X4 = X4_all, y4 = y4_all, X4_test = X4_test_all, y4_test = y4_test_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'calculation' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-7ba8c43bb4c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcalculation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX4_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my4_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX4_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX4_test_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my4_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my4_test_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'calculation' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}