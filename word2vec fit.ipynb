{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'bank_train'\n",
    "df = pd.read_csv(filename+'.csv')\n",
    "df_test = pd.read_csv('bank_test_etalon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('russian')\n",
    "#we can add stop words\n",
    "\n",
    "import pymorphy2\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text_list = nltk.word_tokenize(text)\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    text_list = [word for word in text_list if word[0] != '@' and len(word) > 1 and word.isalpha()]\n",
    "    text_list = [morph.parse(word)[0].normal_form for word in text_list]\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdir = '/home/parkin/Data/'\n",
    "df_list = ['reviews1.csv', 'reviews2.csv', 'spr.csv', 'hospitals.csv', 'kinopoisk_bottom100.csv', 'kinopoisk_top250.csv']\n",
    "text_columns = ['text', 'text', 'text', 'content', 'content', 'content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sentences = {}\n",
    "for df_name, text_column in zip(df_list[-2:], text_columns[-2:]):\n",
    "    df = pd.read_csv(path.join(subdir, df_name))\n",
    "    print (df_name)\n",
    "    print ('~~~~~~~~`')\n",
    "    df_len = len(df[text_column])\n",
    "    result = []\n",
    "    for idx, row in enumerate(df[text_column]):\n",
    "        result.append(tokenize(row))\n",
    "        if idx % 500 == 0:\n",
    "            print ('{0}/{1} {2:.2f}%'.format(idx+1,df_len, 100.0 * idx / df_len))\n",
    "    with open(df_name+'.pkl', 'wb') as fout:\n",
    "        pickle.dump(result, fout, pickle.HIGHEST_PROTOCOL)\n",
    "    all_sentences[df_name] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sentences = {}\n",
    "for file in df_list:\n",
    "    with open(file+'.pkl', 'rb') as fin:\n",
    "        l = pickle.load(fin)\n",
    "    all_sentences[file] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~\n",
      "~~~~~~\n",
      "~~~~~~\n",
      "~~~~~~\n",
      "~~~~~~\n",
      "~~~~~~\n"
     ]
    }
   ],
   "source": [
    "#don't use\n",
    "for v in all_sentences.values():\n",
    "    for idx, row in enumerate(v):\n",
    "        v[idx] = ' '.join(row)\n",
    "    print ('~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WARNING\n",
    "with open('all_sentences.pkl', 'wb') as fout:\n",
    "    pickle.dump(all_sentences, fout, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_sentences = all_sentences['reviews1.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=first_sentences,size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('word2vec_normalize.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('посредственно', 0.6553890705108643),\n",
       " ('средненький', 0.599341630935669),\n",
       " ('суховато', 0.5942553877830505),\n",
       " ('неплохо', 0.5631283521652222),\n",
       " ('стандартно', 0.5505111217498779),\n",
       " ('сносно', 0.5501429438591003),\n",
       " ('съедобный', 0.5392283201217651),\n",
       " ('бледно', 0.5392034649848938),\n",
       " ('сносный', 0.537426233291626),\n",
       " ('недурно', 0.5361967086791992)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('средне')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews2.csv\n",
      "2reviews2.csv\n",
      "spr.csv\n",
      "2spr.csv\n",
      "hospitals.csv\n",
      "2hospitals.csv\n",
      "kinopoisk_bottom100.csv\n",
      "2kinopoisk_bottom100.csv\n",
      "kinopoisk_top250.csv\n",
      "2kinopoisk_top250.csv\n"
     ]
    }
   ],
   "source": [
    "for df_name in df_list[1:]:\n",
    "    sentence = all_sentences[df_name]\n",
    "    print (df_name)\n",
    "    model.train(sentence)\n",
    "    print ('2' + df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('word2vec_normalize.model')\n",
    "model.save_word2vec_format('word2vec_normalize.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = list(model.vocab.keys())\n",
    "companies = ['alfabank', 'gazprom', 'raiffeisen', 'rshb', 'sberbank', 'uralsib', 'vtb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_df(_df, _2vec, corporation):\n",
    "    x_arr = []\n",
    "    y_arr = []\n",
    "    for i in range(len(_df)):\n",
    "        label = _df[corporation][i]\n",
    "        if label == '0' or label == '-1' or label == '1' or label == 0.0 or label == -1.0 or label == 1.0:\n",
    "            x_arr.append(_2vec(_df['text'][i]))\n",
    "            y_arr.append(int(label))\n",
    "    X = np.array(x_arr)\n",
    "    y = np.array(y_arr)\n",
    "    print (len(X))\n",
    "    print (len(y))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence2vec(sentence, model):\n",
    "    token_list = tokenize(sentence)\n",
    "    vocab = list(model.vocab.keys())\n",
    "    vector_size = model.vector_size\n",
    "    token_list = [model[x] if x in vocab else np.zeros(vector_size) for x in token_list ]\n",
    "    if len(token_list) > 0:\n",
    "        return 1.0/len(token_list) * sum(token_list)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "sen_to_vec = lambda x: sentence2vec(x, model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n",
      "536\n"
     ]
    }
   ],
   "source": [
    "X, y = filter_df(df_test, sen_to_vec, companies[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "137\n",
      "150\n",
      "150\n",
      "99\n",
      "99\n",
      "2833\n",
      "2833\n",
      "60\n",
      "60\n",
      "651\n",
      "651\n"
     ]
    }
   ],
   "source": [
    "for companie in companies[1:]:\n",
    "    X_cur, y_cur = filter_df(df_test, sen_to_vec, companie)\n",
    "    X = np.concatenate((X,X_cur), axis=0)\n",
    "    y = np.concatenate((y,y_cur), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4466, 200)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape #train data word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X\n",
    "y_test = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "SVM: \n",
      "fit time = 0m6sec\n",
      "predict time = 0m4sec\n",
      "f-macro: 0.4334506801174829\n",
      "f-micro: 0.771383788625168\n"
     ]
    }
   ],
   "source": [
    "print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "print ('SVM: ')\n",
    "start_time = time.time()\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "t = int(time.time() - start_time)\n",
    "print ('fit time = {0}m{1}sec'.format(t // 60, t % 60))\n",
    "start_time = time.time()\n",
    "y_predict = model.predict(X_test)\n",
    "t = int(time.time() - start_time)\n",
    "print ('predict time = {0}m{1}sec'.format(t // 60, t % 60))\n",
    "print ('f-macro: {0}'.format(f1_score(y_test, y_predict, average = 'macro')))\n",
    "print ('f-micro: {0}'.format(f1_score(y_test, y_predict, average = 'micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange f score - 0.4546572057744066\n"
     ]
    }
   ],
   "source": [
    "import my_score\n",
    "func= lambda x,y: my_score.f_macro(my_score.cound_diff(x,y))\n",
    "print ('strange f score - {0}'.format(func(y_predict, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 275,  389,    0],\n",
       "       [ 299, 3169,    0],\n",
       "       [  49,  284,    1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
